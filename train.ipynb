{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec818278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna tuning notebook (conservative setting)\n",
    "# - Tuning XGBoost and TabNet with Optuna\n",
    "# - Conservative search (A): n_trials = 30\n",
    "# - Follow your pipeline: split 70/15/15, StandardScaler fit on train, SMOTE+Tomek on train only\n",
    "# - Final evaluation uses validation to find threshold and test set for final metrics\n",
    "\n",
    "# Note: This is a single-file script intended to be run as a Jupyter cell-by-cell notebook.\n",
    "# If you want .ipynb, copy this code into a new notebook cell blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b774334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Perkuliahan\\Python\\CICI\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, f1_score, accuracy_score,\n",
    "    precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.utils import check_random_state\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89510297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version : 2.9.1+cu130\n",
      "Using Optuna for Bayesian-style search (TPE) | Conservative setting (n_trials=30)\n",
      "TabNet device set to GPU: NVIDIA GeForce RTX 5060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# === USER EDITABLE CONFIG ===\n",
    "DATA_PATH = \"data/diabetes_binary.csv\"  # <-- sesuaikan\n",
    "SEED = 42\n",
    "N_TRIALS = 30  # conservative (A)\n",
    "N_FOLDS = 3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # TabNet device; XGBoost stays CPU\n",
    "\n",
    "# Reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch Version : {torch.__version__}\")\n",
    "print(\"Using Optuna for Bayesian-style search (TPE) | Conservative setting (n_trials=30)\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"TabNet device set to GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available; TabNet will run on CPU (XGBoost stays on CPU).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751e6b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# === LOAD DATA ===\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "X = df.iloc[:, 1:22].values\n",
    "y = df.iloc[:, 0].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99f8df7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Shape: (177576, 21)\n",
      "Original Class dist : [152834  24742]\n",
      "SMOTE+Tomek Train Shape   : (304175, 21)\n",
      "SMOTE+Tomek Class dist    : [152834 151341]\n"
     ]
    }
   ],
   "source": [
    "# === SPLIT 70/15/15 ===\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=SEED\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=SEED\n",
    ")\n",
    "\n",
    "# === SCALING ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_before_balance = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === BALANCE (SMOTE + TOMEK) on train only ===\n",
    "print(f\"Original Train Shape: {X_train.shape}\")\n",
    "print(f\"Original Class dist : {np.bincount(y_train.astype(int))}\")\n",
    "\n",
    "smt = SMOTETomek(random_state=SEED, smote=SMOTE(random_state=SEED), tomek=TomekLinks())\n",
    "X_train_bal, y_train_bal = smt.fit_resample(X_train_scaled_before_balance, y_train)\n",
    "\n",
    "print(f\"SMOTE+Tomek Train Shape   : {X_train_bal.shape}\")\n",
    "print(f\"SMOTE+Tomek Class dist    : {np.bincount(y_train_bal.astype(int))}\")\n",
    "\n",
    "# Single stratified split of the balanced train for tuning (train/valid)\n",
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(\n",
    "    X_train_bal, y_train_bal, test_size=0.20, stratify=y_train_bal, random_state=SEED\n",
    ")\n",
    "\n",
    "# Utility: threshold search on validation\n",
    "def find_best_threshold(y_val, y_prob_val):\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    f1s = [f1_score(y_val, (y_prob_val > th).astype(int)) for th in thresholds]\n",
    "    best_th = thresholds[np.argmax(f1s)]\n",
    "    return best_th\n",
    "\n",
    "# Metrics container\n",
    "from collections import OrderedDict\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    sensitivity = recall\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) != 0 else 0\n",
    "    return OrderedDict({\n",
    "        'Accuracy': acc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': auc,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62865bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTUNA OBJECTIVE FOR XGBOOST ===\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    # Suggest hyperparameters\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 200, 300]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': 1  # limit inside trials to avoid oversubscription\n",
    "    }\n",
    "\n",
    "    clf = XGBClassifier(**param)\n",
    "    clf.fit(X_train_tune, y_train_tune, verbose=False)\n",
    "    y_pred = clf.predict(X_val_tune)\n",
    "    return float(f1_score(y_val_tune, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "642db2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTUNA OBJECTIVE FOR TABNET ===\n",
    "\n",
    "def objective_tabnet(trial):\n",
    "    # Suggest hyperparameters (conservative ranges)\n",
    "    n_d = trial.suggest_categorical('n_d', [8, 16, 32])\n",
    "    n_a = trial.suggest_categorical('n_a', [8, 16, 32])\n",
    "    n_steps = trial.suggest_int('n_steps', 3, 8)\n",
    "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-4, 1e-2)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-3, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
    "\n",
    "    max_epochs = 50\n",
    "    patience = 10\n",
    "\n",
    "    clf = TabNetClassifier(\n",
    "        n_d=n_d, n_a=n_a, n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=lr),\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        scheduler_params={\"step_size\": 10, \"gamma\": 0.5},\n",
    "        mask_type='entmax',\n",
    "        device_name=DEVICE,\n",
    "        seed=SEED,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train=X_train_tune, y_train=y_train_tune,\n",
    "        eval_set=[(X_val_tune, y_val_tune)],\n",
    "        eval_name=['valid'],\n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=patience,\n",
    "        batch_size=batch_size,\n",
    "        virtual_batch_size=min(64, batch_size),\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    y_prob = clf.predict_proba(X_val_tune)[:, 1]\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    score = float(f1_score(y_val_tune, y_pred))\n",
    "\n",
    "    del clf\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d12294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 00:34:23,553] A new study created in memory with name: no-name-97185821-e995-4e47-ae07-c3670365359e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 00:34:31,047] Trial 0 finished with value: 0.8866379310344827 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.015958237752949748, 'subsample': 0.662397808134481, 'colsample_bytree': 0.6232334448672797, 'reg_alpha': 0.8661761457749352, 'reg_lambda': 0.6011150117432088}. Best is trial 0 with value: 0.8866379310344827.\n",
      "[I 2025-12-10 00:34:39,467] Trial 1 finished with value: 0.9069413314626927 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.018891200276189388, 'subsample': 0.6727299868828402, 'colsample_bytree': 0.6733618039413735, 'reg_alpha': 0.3042422429595377, 'reg_lambda': 0.5247564316322378}. Best is trial 1 with value: 0.9069413314626927.\n",
      "[I 2025-12-10 00:34:44,935] Trial 2 finished with value: 0.8873661100392523 and parameters: {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.023993242906812727, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.7824279936868144, 'reg_alpha': 0.7851759613930136, 'reg_lambda': 0.19967378215835974}. Best is trial 1 with value: 0.9069413314626927.\n",
      "[I 2025-12-10 00:34:50,088] Trial 3 finished with value: 0.8875136874937785 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.016666983286066417, 'subsample': 0.6260206371941118, 'colsample_bytree': 0.9795542149013333, 'reg_alpha': 0.9656320330745594, 'reg_lambda': 0.8083973481164611}. Best is trial 1 with value: 0.9069413314626927.\n",
      "[I 2025-12-10 00:34:56,469] Trial 4 finished with value: 0.8898396790250886 and parameters: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.014413697528610409, 'subsample': 0.798070764044508, 'colsample_bytree': 0.6137554084460873, 'reg_alpha': 0.9093204020787821, 'reg_lambda': 0.2587799816000169}. Best is trial 1 with value: 0.9069413314626927.\n",
      "[I 2025-12-10 00:34:59,285] Trial 5 finished with value: 0.8593408742706262 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.017398074711291726, 'subsample': 0.9878338511058234, 'colsample_bytree': 0.9100531293444458, 'reg_alpha': 0.9394989415641891, 'reg_lambda': 0.8948273504276488}. Best is trial 1 with value: 0.9069413314626927.\n",
      "[I 2025-12-10 00:35:03,750] Trial 6 finished with value: 0.8380373425966131 and parameters: {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.011450964268326641, 'subsample': 0.7301321323053057, 'colsample_bytree': 0.7554709158757928, 'reg_alpha': 0.2713490317738959, 'reg_lambda': 0.8287375091519293}. Best is trial 1 with value: 0.9069413314626927.\n",
      "[I 2025-12-10 00:35:09,144] Trial 7 finished with value: 0.9128910134510372 and parameters: {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.11058146376563001, 'subsample': 0.6298202574719083, 'colsample_bytree': 0.9947547746402069, 'reg_alpha': 0.7722447692966574, 'reg_lambda': 0.1987156815341724}. Best is trial 7 with value: 0.9128910134510372.\n",
      "[I 2025-12-10 00:35:14,279] Trial 8 finished with value: 0.914325573284506 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.10079659367870726, 'subsample': 0.6296178606936361, 'colsample_bytree': 0.7433862914177091, 'reg_alpha': 0.11586905952512971, 'reg_lambda': 0.8631034258755935}. Best is trial 8 with value: 0.914325573284506.\n",
      "[I 2025-12-10 00:35:16,621] Trial 9 finished with value: 0.8608588479501816 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.026489469207362663, 'subsample': 0.8918424713352255, 'colsample_bytree': 0.8550229885420852, 'reg_alpha': 0.8872127425763265, 'reg_lambda': 0.4722149251619493}. Best is trial 8 with value: 0.914325573284506.\n",
      "[I 2025-12-10 00:35:23,217] Trial 10 finished with value: 0.9138411961399192 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.18255631599020644, 'subsample': 0.8787844944027112, 'colsample_bytree': 0.7151987250787625, 'reg_alpha': 0.03872519996118318, 'reg_lambda': 0.9722285321722851}. Best is trial 8 with value: 0.914325573284506.\n",
      "[I 2025-12-10 00:35:29,810] Trial 11 finished with value: 0.9140073841673745 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.19854668872060813, 'subsample': 0.887352152856701, 'colsample_bytree': 0.707897806810714, 'reg_alpha': 0.02387042599324944, 'reg_lambda': 0.991626047207494}. Best is trial 8 with value: 0.914325573284506.\n",
      "[I 2025-12-10 00:35:35,363] Trial 12 finished with value: 0.9149058477189193 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.07898927623865827, 'subsample': 0.8854232900266661, 'colsample_bytree': 0.7067964291781478, 'reg_alpha': 0.0037214867687368723, 'reg_lambda': 0.7212484584766519}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:35:40,401] Trial 13 finished with value: 0.9134019901189896 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.06927739377249721, 'subsample': 0.9894703496052201, 'colsample_bytree': 0.8479803400683418, 'reg_alpha': 0.20809611239458475, 'reg_lambda': 0.6738641489405021}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:35:45,887] Trial 14 finished with value: 0.912925736902406 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.05262928641976177, 'subsample': 0.8252227744012373, 'colsample_bytree': 0.675690572721106, 'reg_alpha': 0.4996179711508223, 'reg_lambda': 0.7096414508081775}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:35:50,949] Trial 15 finished with value: 0.9143394714377798 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.09390300473102578, 'subsample': 0.9337585570065022, 'colsample_bytree': 0.8189931879748622, 'reg_alpha': 0.14703253967981714, 'reg_lambda': 0.4273264241382153}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:35:54,136] Trial 16 finished with value: 0.8985302049622438 and parameters: {'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.03876949114617207, 'subsample': 0.9429564520037396, 'colsample_bytree': 0.8318264197864259, 'reg_alpha': 0.3716653158980562, 'reg_lambda': 0.3739777057415845}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:35:59,213] Trial 17 finished with value: 0.9142010803275832 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.09341897073826798, 'subsample': 0.9353523847892582, 'colsample_bytree': 0.80244769856573, 'reg_alpha': 0.1605254668052258, 'reg_lambda': 0.020913566483403923}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:03,494] Trial 18 finished with value: 0.9102086760758508 and parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.06588331932770174, 'subsample': 0.8307796695558711, 'colsample_bytree': 0.9032799400776628, 'reg_alpha': 0.4401207085272357, 'reg_lambda': 0.3807495883998396}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:06,761] Trial 19 finished with value: 0.9145926028921023 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.12690838772957386, 'subsample': 0.9344545337355245, 'colsample_bytree': 0.6598102503029603, 'reg_alpha': 0.6246461037758938, 'reg_lambda': 0.7231179398161656}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:10,048] Trial 20 finished with value: 0.9143046058011867 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.13940127601485924, 'subsample': 0.866415580919191, 'colsample_bytree': 0.6733221423256304, 'reg_alpha': 0.5818204903272258, 'reg_lambda': 0.717338461745671}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:12,978] Trial 21 finished with value: 0.9140400647440695 and parameters: {'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.1342861978375931, 'subsample': 0.9349423829921533, 'colsample_bytree': 0.6012991841109346, 'reg_alpha': 0.6387604997562217, 'reg_lambda': 0.5601434914481938}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:15,695] Trial 22 finished with value: 0.9099803631102077 and parameters: {'n_estimators': 100, 'max_depth': 8, 'learning_rate': 0.0828370053541923, 'subsample': 0.9207623173607457, 'colsample_bytree': 0.6368484391673688, 'reg_alpha': 0.6640057730611126, 'reg_lambda': 0.44414001934246317}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:19,171] Trial 23 finished with value: 0.9050336427902222 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.04396840250147844, 'subsample': 0.9640696014897872, 'colsample_bytree': 0.716765191776162, 'reg_alpha': 0.10126299231159261, 'reg_lambda': 0.6271047376319923}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:24,744] Trial 24 finished with value: 0.9136004171373946 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.06318343165940399, 'subsample': 0.8436388599888756, 'colsample_bytree': 0.7824416011209641, 'reg_alpha': 0.0005948554850835341, 'reg_lambda': 0.7563337368618012}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:26,852] Trial 25 finished with value: 0.890695940909399 and parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.13903370323147818, 'subsample': 0.7787094917665793, 'colsample_bytree': 0.6523648133987512, 'reg_alpha': 0.23939534728504402, 'reg_lambda': 0.3216010417581228}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:32,437] Trial 26 finished with value: 0.9135776650807759 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.08157006645959414, 'subsample': 0.9030617688370578, 'colsample_bytree': 0.7461087814572279, 'reg_alpha': 0.396223822631122, 'reg_lambda': 0.6405301333656299}. Best is trial 12 with value: 0.9149058477189193.\n",
      "[I 2025-12-10 00:36:38,986] Trial 27 finished with value: 0.9155512290094914 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.1179380942098765, 'subsample': 0.9621945045088368, 'colsample_bytree': 0.8900646198797656, 'reg_alpha': 0.5030911215349643, 'reg_lambda': 0.7591065325083481}. Best is trial 27 with value: 0.9155512290094914.\n",
      "[I 2025-12-10 00:36:48,640] Trial 28 finished with value: 0.9143611386095625 and parameters: {'n_estimators': 300, 'max_depth': 10, 'learning_rate': 0.11974037695176158, 'subsample': 0.9978632018252712, 'colsample_bytree': 0.8901586532870034, 'reg_alpha': 0.5471040613885462, 'reg_lambda': 0.7751563482131618}. Best is trial 27 with value: 0.9155512290094914.\n",
      "[I 2025-12-10 00:36:52,118] Trial 29 finished with value: 0.9148584454227496 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.16148745322437827, 'subsample': 0.9659675926125392, 'colsample_bytree': 0.9419038706180749, 'reg_alpha': 0.7193232732121052, 'reg_lambda': 0.9150910594216407}. Best is trial 27 with value: 0.9155512290094914.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGB trial: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.1179380942098765, 'subsample': 0.9621945045088368, 'colsample_bytree': 0.8900646198797656, 'reg_alpha': 0.5030911215349643, 'reg_lambda': 0.7591065325083481}\n"
     ]
    }
   ],
   "source": [
    "# === RUN STUDIES ===\n",
    "\n",
    "# XGBoost study\n",
    "print(\"Starting Optuna study for XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS, n_jobs=1)\n",
    "\n",
    "print(\"Best XGB trial:\", study_xgb.best_trial.params)\n",
    "\n",
    "# Save XGBoost study results\n",
    "with open('optuna_xgb_best.json', 'w') as f:\n",
    "    json.dump(study_xgb.best_trial.params, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24599ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost final trained in 0.02 mins\n",
      "\n",
      "--- XGBoost Metrics ---\n",
      "Accuracy    : 0.8105\n",
      "Precision   : 0.3800\n",
      "Recall      : 0.5696\n",
      "F1-Score    : 0.4559\n",
      "ROC-AUC     : 0.8203\n",
      "Sensitivity : 0.5696\n",
      "Specificity : 0.8496\n",
      "PPV         : 0.3800\n",
      "NPV         : 0.9242\n"
     ]
    }
   ],
   "source": [
    "# === TRAIN FINAL XGBOOST MODEL AND EVALUATE ===\n",
    "\n",
    "best_xgb_params = study_xgb.best_trial.params\n",
    "# map types: ensure required keys exist\n",
    "xgb_final = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED,\n",
    "    **best_xgb_params\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "xgb_final.fit(X_train_bal, y_train_bal)\n",
    "end = time.time()\n",
    "print(f\"XGBoost final trained in {(end-start)/60:.2f} mins\")\n",
    "\n",
    "# Find best threshold on validation\n",
    "xgb_prob_val = xgb_final.predict_proba(X_val_scaled)[:, 1]\n",
    "xgb_best_th = find_best_threshold(y_val, xgb_prob_val)\n",
    "\n",
    "# Predict on test\n",
    "y_prob_xgb = xgb_final.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_xgb = (y_prob_xgb > xgb_best_th).astype(int)\n",
    "\n",
    "xgb_metrics = calculate_metrics(y_test, y_pred_xgb, y_prob_xgb)\n",
    "print('\\n--- XGBoost Metrics ---')\n",
    "for k, v in xgb_metrics.items():\n",
    "    print(f\"{k:<12}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60917aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 00:36:53,510] A new study created in memory with name: no-name-f5ac8d32-a71c-461c-8de4-cb9f1df7131f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study for TabNet... (this may take a while)\n",
      "\n",
      "Early stopping occurred at epoch 21 with best_epoch = 11 and best_valid_auc = 0.8906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 00:47:02,651] Trial 0 finished with value: 0.8075998774213319 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 3, 'gamma': 1.866176145774935, 'lambda_sparse': 0.0015930522616241021, 'lr': 0.02607024758370768, 'batch_size': 256}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_valid_auc = 0.88324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 01:00:09,453] Trial 1 finished with value: 0.7993415795697894 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 4, 'gamma': 1.6118528947223796, 'lambda_sparse': 0.00019010245319870352, 'lr': 0.00383962929980417, 'batch_size': 512}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_valid_auc = 0.89487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 01:06:05,112] Trial 2 finished with value: 0.7992563810544651 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.9488855372533331, 'lambda_sparse': 0.00853618986286683, 'lr': 0.041380401125610165, 'batch_size': 512}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_valid_auc = 0.88773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 01:21:38,758] Trial 3 finished with value: 0.8000579588813936 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 6, 'gamma': 1.311711076089411, 'lambda_sparse': 0.001096821720752952, 'lr': 0.0123999678368461, 'batch_size': 256}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_valid_auc = 0.87219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 01:43:38,647] Trial 4 finished with value: 0.7904834007184163 and parameters: {'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.3253303307632645, 'lambda_sparse': 0.0005989003672254305, 'lr': 0.003488976654890368, 'batch_size': 128}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_valid_auc = 0.87755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 01:53:05,808] Trial 5 finished with value: 0.7962465238398558 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 4, 'gamma': 1.0055221171236024, 'lambda_sparse': 0.004274869455295219, 'lr': 0.025924756604751596, 'batch_size': 256}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_valid_auc = 0.8651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 02:08:27,931] Trial 6 finished with value: 0.7874587668104542 and parameters: {'n_d': 32, 'n_a': 8, 'n_steps': 4, 'gamma': 1.325183322026747, 'lambda_sparse': 0.002878805718308925, 'lr': 0.018841476921545086, 'batch_size': 128}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 50 with best_epoch = 48 and best_valid_auc = 0.86641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 02:58:38,635] Trial 7 finished with value: 0.7936990765888104 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 5, 'gamma': 1.025419126744095, 'lambda_sparse': 0.00016435497475111326, 'lr': 0.001155735281626987, 'batch_size': 128}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_valid_auc = 0.86873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 03:07:08,747] Trial 8 finished with value: 0.7771093273557383 and parameters: {'n_d': 8, 'n_a': 8, 'n_steps': 4, 'gamma': 1.1612212872540044, 'lambda_sparse': 0.007234279845665418, 'lr': 0.04132765459466366, 'batch_size': 256}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_valid_auc = 0.88317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 03:18:30,176] Trial 9 finished with value: 0.7949607626953759 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2279351625419417, 'lambda_sparse': 0.0007148510793512986, 'lr': 0.04325432427964557, 'batch_size': 128}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_valid_auc = 0.88693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 03:44:32,821] Trial 10 finished with value: 0.7960036868786622 and parameters: {'n_d': 16, 'n_a': 32, 'n_steps': 8, 'gamma': 1.9541218213108225, 'lambda_sparse': 0.0017556088263664706, 'lr': 0.005762168055965823, 'batch_size': 256}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_valid_auc = 0.88885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 04:03:46,018] Trial 11 finished with value: 0.8030990415335463 and parameters: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.643811744770019, 'lambda_sparse': 0.0013093310334961429, 'lr': 0.012151469945526802, 'batch_size': 256}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 18 with best_epoch = 8 and best_valid_auc = 0.87497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 04:18:40,567] Trial 12 finished with value: 0.7949600473064152 and parameters: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.7162871820090468, 'lambda_sparse': 0.0003419653838206277, 'lr': 0.007827949812496831, 'batch_size': 256}. Best is trial 0 with value: 0.8075998774213319.\n"
     ]
    }
   ],
   "source": [
    "# === RUN TABNET STUDY ===\n",
    "print(\"Starting Optuna study for TabNet... (this may take a while)\")\n",
    "study_tab = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "study_tab.optimize(objective_tabnet, n_trials=N_TRIALS, n_jobs=1)\n",
    "\n",
    "print(\"Best TabNet trial:\", study_tab.best_trial.params)\n",
    "\n",
    "# Save TabNet study results\n",
    "with open('optuna_tabnet_best.json', 'w') as f:\n",
    "    json.dump(study_tab.best_trial.params, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada76c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TRAIN FINAL TABNET MODEL AND EVALUATE ===\n",
    "\n",
    "best_tab = study_tab.best_trial.params\n",
    "clf_tabnet_final = TabNetClassifier(\n",
    "    n_d=best_tab.get('n_d', 8),\n",
    "    n_a=best_tab.get('n_a', 8),\n",
    "    n_steps=best_tab.get('n_steps', 3),\n",
    "    gamma=best_tab.get('gamma', 1.3),\n",
    "    lambda_sparse=best_tab.get('lambda_sparse', 0.001),\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=best_tab.get('lr', 0.02)),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    scheduler_params={\"step_size\": 10, \"gamma\": 0.5},\n",
    "    mask_type='entmax',\n",
    "    device_name=DEVICE,\n",
    "    seed=SEED,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "clf_tabnet_final.fit(\n",
    "    X_train=X_train_bal, y_train=y_train_bal,\n",
    "    eval_set=[(X_train_bal, y_train_bal), (X_val_scaled, y_val)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['auc', 'accuracy'],\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=best_tab.get('batch_size', 256),\n",
    "    virtual_batch_size=min(128, best_tab.get('batch_size', 256)),\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"TabNet final trained in {(end-start)/60:.2f} mins\")\n",
    "\n",
    "# Evaluate TabNet\n",
    "tab_prob_val = clf_tabnet_final.predict_proba(X_val_scaled)[:, 1]\n",
    "tab_best_th = find_best_threshold(y_val, tab_prob_val)\n",
    "\n",
    "y_prob_tab = clf_tabnet_final.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_tab = (y_prob_tab > tab_best_th).astype(int)\n",
    "\n",
    "tab_metrics = calculate_metrics(y_test, y_pred_tab, y_prob_tab)\n",
    "print('\\n--- TabNet Metrics ---')\n",
    "for k, v in tab_metrics.items():\n",
    "    print(f\"{k:<12}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45956d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Summary DataFrame ---\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'TabNet'],\n",
    "    'Accuracy': [xgb_metrics['Accuracy'], tab_metrics['Accuracy']],\n",
    "    'Precision': [xgb_metrics['Precision'], tab_metrics['Precision']],\n",
    "    'Recall': [xgb_metrics['Recall'], tab_metrics['Recall']],\n",
    "    'F1-Score': [xgb_metrics['F1-Score'], tab_metrics['F1-Score']],\n",
    "    'ROC-AUC': [xgb_metrics['ROC-AUC'], tab_metrics['ROC-AUC']],\n",
    "    'Sensitivity': [xgb_metrics['Sensitivity'], tab_metrics['Sensitivity']],\n",
    "    'Specificity': [xgb_metrics['Specificity'], tab_metrics['Specificity']],\n",
    "    'PPV': [xgb_metrics['PPV'], tab_metrics['PPV']],\n",
    "    'NPV': [xgb_metrics['NPV'], tab_metrics['NPV']]\n",
    "})\n",
    "\n",
    "print('\\n--- Summary ---')\n",
    "print(metrics_df)\n",
    "\n",
    "# Save metrics\n",
    "metrics_df.to_csv('tuning_metrics_summary.csv', index=False)\n",
    "\n",
    "print('\\nSaved: optuna_xgb_best.json, optuna_tabnet_best.json, tuning_metrics_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
