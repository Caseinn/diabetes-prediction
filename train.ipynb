{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec818278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna tuning notebook (conservative setting)\n",
    "# - Tuning XGBoost and TabNet with Optuna\n",
    "# - Conservative search (A): n_trials = 30\n",
    "# - Follow your pipeline: split 70/15/15, StandardScaler fit on train, SMOTE+Tomek on train only\n",
    "# - Final evaluation uses validation to find threshold and test set for final metrics\n",
    "\n",
    "# Note: This is a single-file script intended to be run as a Jupyter cell-by-cell notebook.\n",
    "# If you want .ipynb, copy this code into a new notebook cell blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b961729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting optuna\n",
      "  Using cached optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting xgboost\n",
      "  Using cached xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting pytorch_tabnet\n",
      "  Using cached pytorch_tabnet-4.1.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting imblearn\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl.metadata (355 bytes)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venve/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=0.8.5 (from torch)\n",
      "  Using cached fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch)\n",
      "  Downloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting alembic>=1.5.0 (from optuna)\n",
      "  Using cached alembic-1.17.2-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting colorlog (from optuna)\n",
      "  Using cached colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venve/lib/python3.12/site-packages (from optuna) (25.0)\n",
      "Collecting sqlalchemy>=1.4.2 (from optuna)\n",
      "  Using cached sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (9.5 kB)\n",
      "Collecting tqdm (from optuna)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting PyYAML (from optuna)\n",
      "  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)\n",
      "Collecting scipy (from xgboost)\n",
      "  Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Using cached imbalanced_learn-0.14.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting Mako (from alembic>=1.5.0->optuna)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./venve/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting greenlet>=1 (from sqlalchemy>=1.4.2->optuna)\n",
      "  Using cached greenlet-3.3.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
      "Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "Downloading torch-2.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.5.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "Using cached xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
      "Using cached pytorch_tabnet-4.1.0-py3-none-any.whl (44 kB)\n",
      "Using cached scikit_learn-1.7.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.5 MB)\n",
      "Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached alembic-1.17.2-py3-none-any.whl (248 kB)\n",
      "Using cached fsspec-2025.12.0-py3-none-any.whl (201 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.7 MB)\n",
      "Using cached sqlalchemy-2.0.45-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.3 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Using cached imbalanced_learn-0.14.0-py3-none-any.whl (239 kB)\n",
      "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (807 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached greenlet-3.3.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (609 kB)\n",
      "Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: pytz, nvidia-cusparselt-cu12, mpmath, tzdata, typing-extensions, triton, tqdm, threadpoolctl, sympy, setuptools, PyYAML, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, joblib, greenlet, fsspec, filelock, colorlog, sqlalchemy, scipy, pandas, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, Mako, jinja2, xgboost, scikit-learn, nvidia-cusolver-cu12, alembic, torch, optuna, imbalanced-learn, pytorch_tabnet, imblearn\n",
      "Successfully installed Mako-1.3.10 MarkupSafe-3.0.3 PyYAML-6.0.3 alembic-1.17.2 colorlog-6.10.1 filelock-3.20.0 fsspec-2025.12.0 greenlet-3.3.0 imbalanced-learn-0.14.0 imblearn-0.0 jinja2-3.1.6 joblib-1.5.2 mpmath-1.3.0 networkx-3.6.1 numpy-2.3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 optuna-4.6.0 pandas-2.3.3 pytorch_tabnet-4.1.0 pytz-2025.2 scikit-learn-1.7.2 scipy-1.16.3 setuptools-80.9.0 sqlalchemy-2.0.45 sympy-1.14.0 threadpoolctl-3.6.0 torch-2.9.1 tqdm-4.67.1 triton-3.5.1 typing-extensions-4.15.0 tzdata-2025.2 xgboost-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas torch optuna xgboost pytorch_tabnet scikit-learn imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b774334",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/diabetes-prediction/venve/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, f1_score, accuracy_score,\n",
    "    precision_score, recall_score, confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.utils import check_random_state\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89510297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version : 2.9.1+cu128\n",
      "Using Optuna for Bayesian-style search (TPE) | Conservative setting (n_trials=30)\n",
      "TabNet device set to GPU: NVIDIA GeForce RTX 5090\n"
     ]
    }
   ],
   "source": [
    "# === USER EDITABLE CONFIG ===\n",
    "DATA_PATH = \"data/diabetes_binary.csv\"  # <-- sesuaikan\n",
    "SEED = 42\n",
    "N_TRIALS = 30  # conservative (A)\n",
    "N_FOLDS = 3\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'  # TabNet device; XGBoost stays CPU\n",
    "\n",
    "# Reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(f\"PyTorch Version : {torch.__version__}\")\n",
    "print(\"Using Optuna for Bayesian-style search (TPE) | Conservative setting (n_trials=30)\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"TabNet device set to GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"CUDA not available; TabNet will run on CPU (XGBoost stays on CPU).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "751e6b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "# === LOAD DATA ===\n",
    "print(\"Loading data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "X = df.iloc[:, 1:22].values\n",
    "y = df.iloc[:, 0].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f8df7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Train Shape: (177576, 21)\n",
      "Original Class dist : [152834  24742]\n",
      "SMOTE+Tomek Train Shape   : (304173, 21)\n",
      "SMOTE+Tomek Class dist    : [152834 151339]\n"
     ]
    }
   ],
   "source": [
    "# === SPLIT 70/15/15 ===\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.30, stratify=y, random_state=SEED\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=SEED\n",
    ")\n",
    "\n",
    "# === SCALING ===\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_before_balance = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# === BALANCE (SMOTE + TOMEK) on train only ===\n",
    "print(f\"Original Train Shape: {X_train.shape}\")\n",
    "print(f\"Original Class dist : {np.bincount(y_train.astype(int))}\")\n",
    "\n",
    "smt = SMOTETomek(random_state=SEED, smote=SMOTE(random_state=SEED), tomek=TomekLinks())\n",
    "X_train_bal, y_train_bal = smt.fit_resample(X_train_scaled_before_balance, y_train)\n",
    "\n",
    "print(f\"SMOTE+Tomek Train Shape   : {X_train_bal.shape}\")\n",
    "print(f\"SMOTE+Tomek Class dist    : {np.bincount(y_train_bal.astype(int))}\")\n",
    "\n",
    "# Single stratified split of the balanced train for tuning (train/valid)\n",
    "X_train_tune, X_val_tune, y_train_tune, y_val_tune = train_test_split(\n",
    "    X_train_bal, y_train_bal, test_size=0.20, stratify=y_train_bal, random_state=SEED\n",
    ")\n",
    "\n",
    "# Utility: threshold search on validation\n",
    "def find_best_threshold(y_val, y_prob_val):\n",
    "    thresholds = np.arange(0.1, 0.9, 0.01)\n",
    "    f1s = [f1_score(y_val, (y_prob_val > th).astype(int)) for th in thresholds]\n",
    "    best_th = thresholds[np.argmax(f1s)]\n",
    "    return best_th\n",
    "\n",
    "# Metrics container\n",
    "from collections import OrderedDict\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    sensitivity = recall\n",
    "    specificity = tn / (tn + fp) if (tn + fp) != 0 else 0\n",
    "    ppv = tp / (tp + fp) if (tp + fp) != 0 else 0\n",
    "    npv = tn / (tn + fn) if (tn + fn) != 0 else 0\n",
    "    return OrderedDict({\n",
    "        'Accuracy': acc,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': auc,\n",
    "        'Sensitivity': sensitivity,\n",
    "        'Specificity': specificity,\n",
    "        'PPV': ppv,\n",
    "        'NPV': npv\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62865bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTUNA OBJECTIVE FOR XGBOOST ===\n",
    "\n",
    "def objective_xgb(trial):\n",
    "    # Suggest hyperparameters\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [100, 200, 300]),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 1.0),\n",
    "        'objective': 'binary:logistic',\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': SEED,\n",
    "        'n_jobs': 1  # limit inside trials to avoid oversubscription\n",
    "    }\n",
    "\n",
    "    clf = XGBClassifier(**param)\n",
    "    clf.fit(X_train_tune, y_train_tune, verbose=False)\n",
    "    y_pred = clf.predict(X_val_tune)\n",
    "    return float(f1_score(y_val_tune, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "642db2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTUNA OBJECTIVE FOR TABNET ===\n",
    "\n",
    "def objective_tabnet(trial):\n",
    "    # Suggest hyperparameters (conservative ranges)\n",
    "    n_d = trial.suggest_categorical('n_d', [8, 16, 32])\n",
    "    n_a = trial.suggest_categorical('n_a', [8, 16, 32])\n",
    "    n_steps = trial.suggest_int('n_steps', 3, 8)\n",
    "    gamma = trial.suggest_float('gamma', 1.0, 2.0)\n",
    "    lambda_sparse = trial.suggest_loguniform('lambda_sparse', 1e-4, 1e-2)\n",
    "    lr = trial.suggest_loguniform('lr', 1e-3, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [128, 256, 512])\n",
    "\n",
    "    max_epochs = 50\n",
    "    patience = 10\n",
    "\n",
    "    clf = TabNetClassifier(\n",
    "        n_d=n_d, n_a=n_a, n_steps=n_steps,\n",
    "        gamma=gamma,\n",
    "        lambda_sparse=lambda_sparse,\n",
    "        optimizer_fn=torch.optim.Adam,\n",
    "        optimizer_params=dict(lr=lr),\n",
    "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "        scheduler_params={\"step_size\": 10, \"gamma\": 0.5},\n",
    "        mask_type='entmax',\n",
    "        device_name=DEVICE,\n",
    "        seed=SEED,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    clf.fit(\n",
    "        X_train=X_train_tune, y_train=y_train_tune,\n",
    "        eval_set=[(X_val_tune, y_val_tune)],\n",
    "        eval_name=['valid'],\n",
    "        eval_metric=['auc'],\n",
    "        max_epochs=max_epochs,\n",
    "        patience=patience,\n",
    "        batch_size=batch_size,\n",
    "        virtual_batch_size=min(64, batch_size),\n",
    "        num_workers=0,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    y_prob = clf.predict_proba(X_val_tune)[:, 1]\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    score = float(f1_score(y_val_tune, y_pred))\n",
    "\n",
    "    del clf\n",
    "    torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d12294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 01:54:06,982] A new study created in memory with name: no-name-39e1abe6-a361-4b21-a9bf-b356f4701da8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study for XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 01:54:11,120] Trial 0 finished with value: 0.885404741758333 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.015958237752949748, 'subsample': 0.662397808134481, 'colsample_bytree': 0.6232334448672797, 'reg_alpha': 0.8661761457749352, 'reg_lambda': 0.6011150117432088}. Best is trial 0 with value: 0.885404741758333.\n",
      "[I 2025-12-10 01:54:18,278] Trial 1 finished with value: 0.9063586984458799 and parameters: {'n_estimators': 300, 'max_depth': 9, 'learning_rate': 0.018891200276189388, 'subsample': 0.6727299868828402, 'colsample_bytree': 0.6733618039413735, 'reg_alpha': 0.3042422429595377, 'reg_lambda': 0.5247564316322378}. Best is trial 1 with value: 0.9063586984458799.\n",
      "[I 2025-12-10 01:54:22,147] Trial 2 finished with value: 0.8877298473497999 and parameters: {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.023993242906812727, 'subsample': 0.7465447373174767, 'colsample_bytree': 0.7824279936868144, 'reg_alpha': 0.7851759613930136, 'reg_lambda': 0.19967378215835974}. Best is trial 1 with value: 0.9063586984458799.\n",
      "[I 2025-12-10 01:54:26,393] Trial 3 finished with value: 0.8866506264000664 and parameters: {'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.016666983286066417, 'subsample': 0.6260206371941118, 'colsample_bytree': 0.9795542149013333, 'reg_alpha': 0.9656320330745594, 'reg_lambda': 0.8083973481164611}. Best is trial 1 with value: 0.9063586984458799.\n",
      "[I 2025-12-10 01:54:31,258] Trial 4 finished with value: 0.8893222005566388 and parameters: {'n_estimators': 300, 'max_depth': 6, 'learning_rate': 0.014413697528610409, 'subsample': 0.798070764044508, 'colsample_bytree': 0.6137554084460873, 'reg_alpha': 0.9093204020787821, 'reg_lambda': 0.2587799816000169}. Best is trial 1 with value: 0.9063586984458799.\n",
      "[I 2025-12-10 01:54:33,515] Trial 5 finished with value: 0.8603169949013667 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.017398074711291726, 'subsample': 0.9878338511058234, 'colsample_bytree': 0.9100531293444458, 'reg_alpha': 0.9394989415641891, 'reg_lambda': 0.8948273504276488}. Best is trial 1 with value: 0.9063586984458799.\n",
      "[I 2025-12-10 01:54:36,527] Trial 6 finished with value: 0.8404318462550331 and parameters: {'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.011450964268326641, 'subsample': 0.7301321323053057, 'colsample_bytree': 0.7554709158757928, 'reg_alpha': 0.2713490317738959, 'reg_lambda': 0.8287375091519293}. Best is trial 1 with value: 0.9063586984458799.\n",
      "[I 2025-12-10 01:54:40,505] Trial 7 finished with value: 0.9127847107077406 and parameters: {'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.11058146376563001, 'subsample': 0.6298202574719083, 'colsample_bytree': 0.9947547746402069, 'reg_alpha': 0.7722447692966574, 'reg_lambda': 0.1987156815341724}. Best is trial 7 with value: 0.9127847107077406.\n",
      "[I 2025-12-10 01:54:44,769] Trial 8 finished with value: 0.9137094804424439 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.10079659367870726, 'subsample': 0.6296178606936361, 'colsample_bytree': 0.7433862914177091, 'reg_alpha': 0.11586905952512971, 'reg_lambda': 0.8631034258755935}. Best is trial 8 with value: 0.9137094804424439.\n",
      "[I 2025-12-10 01:54:46,527] Trial 9 finished with value: 0.8605795693702445 and parameters: {'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.026489469207362663, 'subsample': 0.8918424713352255, 'colsample_bytree': 0.8550229885420852, 'reg_alpha': 0.8872127425763265, 'reg_lambda': 0.4722149251619493}. Best is trial 8 with value: 0.9137094804424439.\n",
      "[I 2025-12-10 01:54:52,603] Trial 10 finished with value: 0.913113644645739 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.1825563159902062, 'subsample': 0.8787844944027112, 'colsample_bytree': 0.7151987250787626, 'reg_alpha': 0.0387251999611829, 'reg_lambda': 0.9722285321722851}. Best is trial 8 with value: 0.9137094804424439.\n",
      "[I 2025-12-10 01:54:58,576] Trial 11 finished with value: 0.9133590304545612 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.19854668872060807, 'subsample': 0.887352152856701, 'colsample_bytree': 0.7078978068107141, 'reg_alpha': 0.023870425993249483, 'reg_lambda': 0.991626047207494}. Best is trial 8 with value: 0.9137094804424439.\n",
      "[I 2025-12-10 01:55:03,175] Trial 12 finished with value: 0.9144639403390775 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.07898927623865823, 'subsample': 0.8854232900266661, 'colsample_bytree': 0.7067964291781478, 'reg_alpha': 0.0037214867687368793, 'reg_lambda': 0.7212484584766519}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:07,287] Trial 13 finished with value: 0.9129958436951116 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.06927739377249717, 'subsample': 0.9894703496052201, 'colsample_bytree': 0.8479803400683418, 'reg_alpha': 0.20809611239458475, 'reg_lambda': 0.6738641489405023}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:11,873] Trial 14 finished with value: 0.9123587693377369 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.05262928641976177, 'subsample': 0.8252227744012373, 'colsample_bytree': 0.675690572721106, 'reg_alpha': 0.4996179711508223, 'reg_lambda': 0.7096414508081774}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:15,957] Trial 15 finished with value: 0.9139130586409527 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.09390300473102578, 'subsample': 0.9337585570065023, 'colsample_bytree': 0.8189931879748622, 'reg_alpha': 0.14703253967981705, 'reg_lambda': 0.4273264241382153}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:18,673] Trial 16 finished with value: 0.8987116761753204 and parameters: {'n_estimators': 100, 'max_depth': 9, 'learning_rate': 0.03876949114617205, 'subsample': 0.9429564520037396, 'colsample_bytree': 0.8318264197864259, 'reg_alpha': 0.3716653158980561, 'reg_lambda': 0.37397770574158445}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:22,730] Trial 17 finished with value: 0.9141581499180299 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.09341897073826798, 'subsample': 0.9353523847892582, 'colsample_bytree': 0.80244769856573, 'reg_alpha': 0.1605254668052258, 'reg_lambda': 0.020913566483403867}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:25,989] Trial 18 finished with value: 0.90934299232249 and parameters: {'n_estimators': 200, 'max_depth': 6, 'learning_rate': 0.0658833193277017, 'subsample': 0.8307796695558711, 'colsample_bytree': 0.8983504104251205, 'reg_alpha': 0.43610885430317126, 'reg_lambda': 0.013179214380923454}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:28,916] Trial 19 finished with value: 0.9138641938795038 and parameters: {'n_estimators': 100, 'max_depth': 10, 'learning_rate': 0.1269083877295738, 'subsample': 0.9351797302203667, 'colsample_bytree': 0.7826746869618546, 'reg_alpha': 0.6167087486201916, 'reg_lambda': 0.08210490446393415}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:33,243] Trial 20 finished with value: 0.9083802538009883 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.040344252216434597, 'subsample': 0.866415580919191, 'colsample_bytree': 0.8945562209577173, 'reg_alpha': 0.00020716535442425344, 'reg_lambda': 0.36507606637305146}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:37,280] Trial 21 finished with value: 0.9135983263598326 and parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.08181072458544392, 'subsample': 0.9349423829921534, 'colsample_bytree': 0.827030569628532, 'reg_alpha': 0.1525420376801504, 'reg_lambda': 0.6733467046544349}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:42,102] Trial 22 finished with value: 0.9140380196365155 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.143850603489133, 'subsample': 0.9443736408206963, 'colsample_bytree': 0.8066177053151307, 'reg_alpha': 0.12903945451471172, 'reg_lambda': 0.44414001934246317}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:46,811] Trial 23 finished with value: 0.9142229888662381 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.14840074229544045, 'subsample': 0.96760454890743, 'colsample_bytree': 0.7537853638628509, 'reg_alpha': 0.23289030646691125, 'reg_lambda': 0.553693697972629}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:51,454] Trial 24 finished with value: 0.9131008613503752 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.051599753979559786, 'subsample': 0.9106496959441612, 'colsample_bytree': 0.7456568360515724, 'reg_alpha': 0.24191203296478211, 'reg_lambda': 0.6097441186273634}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:55:56,779] Trial 25 finished with value: 0.9138024051095526 and parameters: {'n_estimators': 200, 'max_depth': 10, 'learning_rate': 0.139033703231478, 'subsample': 0.9840071074048179, 'colsample_bytree': 0.6544354937702422, 'reg_alpha': 0.34175023161321694, 'reg_lambda': 0.752181936923242}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:56:01,359] Trial 26 finished with value: 0.9140625 and parameters: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.08157006645959405, 'subsample': 0.8485161614734078, 'colsample_bytree': 0.7118720187068273, 'reg_alpha': 0.08999919690937297, 'reg_lambda': 0.5746786656123268}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:56:03,478] Trial 27 finished with value: 0.9088758631485572 and parameters: {'n_estimators': 200, 'max_depth': 3, 'learning_rate': 0.1633687768866911, 'subsample': 0.96205720202726, 'colsample_bytree': 0.765836702637975, 'reg_alpha': 0.2025103992813044, 'reg_lambda': 0.28812888933519193}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:56:05,433] Trial 28 finished with value: 0.9105117829915079 and parameters: {'n_estimators': 100, 'max_depth': 7, 'learning_rate': 0.11622597240919265, 'subsample': 0.7924257357633805, 'colsample_bytree': 0.725066473699037, 'reg_alpha': 0.6422676233260872, 'reg_lambda': 0.5474081215158431}. Best is trial 12 with value: 0.9144639403390775.\n",
      "[I 2025-12-10 01:56:10,326] Trial 29 finished with value: 0.9134264831580232 and parameters: {'n_estimators': 300, 'max_depth': 7, 'learning_rate': 0.05803541923562271, 'subsample': 0.9676375065708497, 'colsample_bytree': 0.6376970734502531, 'reg_alpha': 0.07308976312079987, 'reg_lambda': 0.6313757746250711}. Best is trial 12 with value: 0.9144639403390775.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGB trial: {'n_estimators': 200, 'max_depth': 9, 'learning_rate': 0.07898927623865823, 'subsample': 0.8854232900266661, 'colsample_bytree': 0.7067964291781478, 'reg_alpha': 0.0037214867687368793, 'reg_lambda': 0.7212484584766519}\n"
     ]
    }
   ],
   "source": [
    "# === RUN STUDIES ===\n",
    "\n",
    "# XGBoost study\n",
    "print(\"Starting Optuna study for XGBoost...\")\n",
    "study_xgb = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "study_xgb.optimize(objective_xgb, n_trials=N_TRIALS, n_jobs=1)\n",
    "\n",
    "print(\"Best XGB trial:\", study_xgb.best_trial.params)\n",
    "\n",
    "# Save XGBoost study results\n",
    "with open('optuna_xgb_best.json', 'w') as f:\n",
    "    json.dump(study_xgb.best_trial.params, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24599ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost final trained in 0.03 mins\n",
      "\n",
      "--- XGBoost Metrics ---\n",
      "Accuracy    : 0.8173\n",
      "Precision   : 0.3911\n",
      "Recall      : 0.5588\n",
      "F1-Score    : 0.4601\n",
      "ROC-AUC     : 0.8265\n",
      "Sensitivity : 0.5588\n",
      "Specificity : 0.8591\n",
      "PPV         : 0.3911\n",
      "NPV         : 0.9232\n"
     ]
    }
   ],
   "source": [
    "# === TRAIN FINAL XGBOOST MODEL AND EVALUATE ===\n",
    "\n",
    "best_xgb_params = study_xgb.best_trial.params\n",
    "# map types: ensure required keys exist\n",
    "xgb_final = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    tree_method='hist',\n",
    "    n_jobs=-1,\n",
    "    random_state=SEED,\n",
    "    **best_xgb_params\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "xgb_final.fit(X_train_bal, y_train_bal)\n",
    "end = time.time()\n",
    "print(f\"XGBoost final trained in {(end-start)/60:.2f} mins\")\n",
    "\n",
    "# Find best threshold on validation\n",
    "xgb_prob_val = xgb_final.predict_proba(X_val_scaled)[:, 1]\n",
    "xgb_best_th = find_best_threshold(y_val, xgb_prob_val)\n",
    "\n",
    "# Predict on test\n",
    "y_prob_xgb = xgb_final.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_xgb = (y_prob_xgb > xgb_best_th).astype(int)\n",
    "\n",
    "xgb_metrics = calculate_metrics(y_test, y_pred_xgb, y_prob_xgb)\n",
    "print('\\n--- XGBoost Metrics ---')\n",
    "for k, v in xgb_metrics.items():\n",
    "    print(f\"{k:<12}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60917aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 01:56:12,317] A new study created in memory with name: no-name-af2873b0-b396-44fb-a3fd-a3c076927db1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Optuna study for TabNet... (this may take a while)\n",
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_valid_auc = 0.84974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 02:03:02,847] Trial 0 finished with value: 0.7816367265469062 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 3, 'gamma': 1.866176145774935, 'lambda_sparse': 0.0015930522616241021, 'lr': 0.02607024758370768, 'batch_size': 256}. Best is trial 0 with value: 0.7816367265469062.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_valid_auc = 0.89003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 02:14:23,142] Trial 1 finished with value: 0.803976721629486 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 4, 'gamma': 1.6118528947223796, 'lambda_sparse': 0.00019010245319870352, 'lr': 0.00383962929980417, 'batch_size': 512}. Best is trial 1 with value: 0.803976721629486.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 15 with best_epoch = 5 and best_valid_auc = 0.85138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 02:18:41,579] Trial 2 finished with value: 0.7812241335199482 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 3, 'gamma': 1.9488855372533331, 'lambda_sparse': 0.00853618986286683, 'lr': 0.041380401125610165, 'batch_size': 512}. Best is trial 1 with value: 0.803976721629486.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 22 with best_epoch = 12 and best_valid_auc = 0.89266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 02:30:14,631] Trial 3 finished with value: 0.8045311109313051 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 6, 'gamma': 1.311711076089411, 'lambda_sparse': 0.001096821720752952, 'lr': 0.0123999678368461, 'batch_size': 256}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_valid_auc = 0.87965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 02:55:12,755] Trial 4 finished with value: 0.800323645132631 and parameters: {'n_d': 8, 'n_a': 8, 'n_steps': 3, 'gamma': 1.3253303307632645, 'lambda_sparse': 0.0005989003672254305, 'lr': 0.003488976654890368, 'batch_size': 128}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_valid_auc = 0.88262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 03:04:10,303] Trial 5 finished with value: 0.7879190837918253 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 4, 'gamma': 1.0055221171236024, 'lambda_sparse': 0.004274869455295219, 'lr': 0.025924756604751596, 'batch_size': 256}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 13 with best_epoch = 3 and best_valid_auc = 0.87773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 03:14:47,380] Trial 6 finished with value: 0.781337375779013 and parameters: {'n_d': 32, 'n_a': 8, 'n_steps': 4, 'gamma': 1.325183322026747, 'lambda_sparse': 0.002878805718308925, 'lr': 0.018841476921545086, 'batch_size': 128}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 50 with best_epoch = 45 and best_valid_auc = 0.86155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 03:56:53,355] Trial 7 finished with value: 0.7915684413556624 and parameters: {'n_d': 16, 'n_a': 8, 'n_steps': 5, 'gamma': 1.025419126744095, 'lambda_sparse': 0.00016435497475111326, 'lr': 0.001155735281626987, 'batch_size': 128}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_valid_auc = 0.86667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 04:03:01,178] Trial 8 finished with value: 0.7885198282394544 and parameters: {'n_d': 8, 'n_a': 8, 'n_steps': 4, 'gamma': 1.1612212872540044, 'lambda_sparse': 0.007234279845665418, 'lr': 0.04132765459466366, 'batch_size': 256}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_valid_auc = 0.86337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 04:10:05,784] Trial 9 finished with value: 0.7867558279929414 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 3, 'gamma': 1.2279351625419417, 'lambda_sparse': 0.0007148510793512986, 'lr': 0.04325432427964557, 'batch_size': 128}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 16 with best_epoch = 6 and best_valid_auc = 0.87959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 04:20:54,601] Trial 10 finished with value: 0.7825954508529651 and parameters: {'n_d': 32, 'n_a': 32, 'n_steps': 8, 'gamma': 1.553009338678487, 'lambda_sparse': 0.0003766982697385595, 'lr': 0.007169292173248393, 'batch_size': 256}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 36 with best_epoch = 26 and best_valid_auc = 0.86922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 04:34:38,360] Trial 11 finished with value: 0.7905990401326594 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 7, 'gamma': 1.6591538956384664, 'lambda_sparse': 0.00010425684126003983, 'lr': 0.00491077928464754, 'batch_size': 512}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 50 with best_epoch = 46 and best_valid_auc = 0.86147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 04:51:30,088] Trial 12 finished with value: 0.7843961616226451 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 6, 'gamma': 1.6411889730588567, 'lambda_sparse': 0.00028118791556313444, 'lr': 0.002050831481117694, 'batch_size': 512}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_valid_auc = 0.88606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 04:59:49,841] Trial 13 finished with value: 0.8003863179074446 and parameters: {'n_d': 8, 'n_a': 32, 'n_steps': 6, 'gamma': 1.4566873541286567, 'lambda_sparse': 0.0014805650074019524, 'lr': 0.012001366433523342, 'batch_size': 512}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 26 with best_epoch = 16 and best_valid_auc = 0.85304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 05:12:14,974] Trial 14 finished with value: 0.7775672028746001 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 5, 'gamma': 1.7421093169083064, 'lambda_sparse': 0.00106675347491872, 'lr': 0.08106561872514696, 'batch_size': 256}. Best is trial 3 with value: 0.8045311109313051.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_valid_auc = 0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 05:20:10,325] Trial 15 finished with value: 0.805995919008005 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.4615242560137316, 'lambda_sparse': 0.000340834329488594, 'lr': 0.010207061814999282, 'batch_size': 512}. Best is trial 15 with value: 0.805995919008005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_valid_auc = 0.89128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 05:33:11,286] Trial 16 finished with value: 0.8021425305375922 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 8, 'gamma': 1.425589464363829, 'lambda_sparse': 0.0004418746102999279, 'lr': 0.010654306865003344, 'batch_size': 256}. Best is trial 15 with value: 0.805995919008005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_valid_auc = 0.88263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 05:39:40,475] Trial 17 finished with value: 0.7939390948126943 and parameters: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.381147871745244, 'lambda_sparse': 0.0027222542866388614, 'lr': 0.007222872275044218, 'batch_size': 512}. Best is trial 15 with value: 0.805995919008005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 27 with best_epoch = 17 and best_valid_auc = 0.88879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 05:50:13,808] Trial 18 finished with value: 0.8020535671041797 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 7, 'gamma': 1.1830330028504403, 'lambda_sparse': 0.0008741010015756288, 'lr': 0.017760091993319797, 'batch_size': 512}. Best is trial 15 with value: 0.805995919008005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_valid_auc = 0.88556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 06:08:19,094] Trial 19 finished with value: 0.8021025713879812 and parameters: {'n_d': 32, 'n_a': 16, 'n_steps': 6, 'gamma': 1.503006709680622, 'lambda_sparse': 0.00027958924928899365, 'lr': 0.0022194960770508185, 'batch_size': 256}. Best is trial 15 with value: 0.805995919008005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_valid_auc = 0.88006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 06:15:58,936] Trial 20 finished with value: 0.7953886777870806 and parameters: {'n_d': 32, 'n_a': 32, 'n_steps': 7, 'gamma': 1.26407036646237, 'lambda_sparse': 0.001420389417169954, 'lr': 0.007336538861985153, 'batch_size': 512}. Best is trial 15 with value: 0.805995919008005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop training because you reached max_epochs = 50 with best_epoch = 46 and best_valid_auc = 0.88328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 06:30:30,996] Trial 21 finished with value: 0.7978654171547572 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 5, 'gamma': 1.5851251024842943, 'lambda_sparse': 0.00017959614396465876, 'lr': 0.0033692195074859635, 'batch_size': 512}. Best is trial 15 with value: 0.805995919008005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 25 with best_epoch = 15 and best_valid_auc = 0.88053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 06:39:16,089] Trial 22 finished with value: 0.796950059644711 and parameters: {'n_d': 8, 'n_a': 16, 'n_steps': 6, 'gamma': 1.812216026432929, 'lambda_sparse': 0.00017535610764708845, 'lr': 0.004817527372759785, 'batch_size': 512}. Best is trial 15 with value: 0.805995919008005.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 24 with best_epoch = 14 and best_valid_auc = 0.90743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 06:49:17,006] Trial 23 finished with value: 0.8095812452221184 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 8, 'gamma': 1.6867971283087997, 'lambda_sparse': 0.000506447356892995, 'lr': 0.017194067952330448, 'batch_size': 512}. Best is trial 23 with value: 0.8095812452221184.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 38 with best_epoch = 28 and best_valid_auc = 0.89393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 07:05:38,446] Trial 24 finished with value: 0.8051161114932364 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 8, 'gamma': 1.7241868218047744, 'lambda_sparse': 0.0004684800139374163, 'lr': 0.015760280245975228, 'batch_size': 512}. Best is trial 23 with value: 0.8095812452221184.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 17 with best_epoch = 7 and best_valid_auc = 0.87858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 07:13:03,013] Trial 25 finished with value: 0.7986006226304596 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 8, 'gamma': 1.7347780293219217, 'lambda_sparse': 0.0004971761357677871, 'lr': 0.017005325920142186, 'batch_size': 512}. Best is trial 23 with value: 0.8095812452221184.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 28 with best_epoch = 18 and best_valid_auc = 0.87371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 07:24:44,040] Trial 26 finished with value: 0.7977499128167028 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 8, 'gamma': 1.9974698284809735, 'lambda_sparse': 0.00029076470956858, 'lr': 0.026912835489301814, 'batch_size': 512}. Best is trial 23 with value: 0.8095812452221184.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_valid_auc = 0.88813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 07:39:27,988] Trial 27 finished with value: 0.8040250488514148 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 8, 'gamma': 1.7229007201722353, 'lambda_sparse': 0.0006748131129379859, 'lr': 0.07164456068536984, 'batch_size': 512}. Best is trial 23 with value: 0.8095812452221184.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 31 with best_epoch = 21 and best_valid_auc = 0.89096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 07:51:14,598] Trial 28 finished with value: 0.8033690509097413 and parameters: {'n_d': 16, 'n_a': 32, 'n_steps': 7, 'gamma': 1.8597022329559751, 'lambda_sparse': 0.0003795559483484729, 'lr': 0.008893984862705692, 'batch_size': 512}. Best is trial 23 with value: 0.8095812452221184.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_valid_auc = 0.8785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-10 08:04:07,677] Trial 29 finished with value: 0.7962417389919579 and parameters: {'n_d': 16, 'n_a': 16, 'n_steps': 8, 'gamma': 1.7999132638568067, 'lambda_sparse': 0.0001197570628800693, 'lr': 0.026953525793715568, 'batch_size': 512}. Best is trial 23 with value: 0.8095812452221184.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best TabNet trial: {'n_d': 16, 'n_a': 16, 'n_steps': 8, 'gamma': 1.6867971283087997, 'lambda_sparse': 0.000506447356892995, 'lr': 0.017194067952330448, 'batch_size': 512}\n"
     ]
    }
   ],
   "source": [
    "# === RUN TABNET STUDY ===\n",
    "print(\"Starting Optuna study for TabNet... (this may take a while)\")\n",
    "study_tab = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "study_tab.optimize(objective_tabnet, n_trials=N_TRIALS, n_jobs=1)\n",
    "\n",
    "print(\"Best TabNet trial:\", study_tab.best_trial.params)\n",
    "\n",
    "# Save TabNet study results\n",
    "with open('optuna_tabnet_best.json', 'w') as f:\n",
    "    json.dump(study_tab.best_trial.params, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ada76c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.60215 | train_auc: 0.82054 | train_accuracy: 0.7452  | valid_auc: 0.79484 | valid_accuracy: 0.71079 |  0:00:32s\n",
      "epoch 1  | loss: 0.50485 | train_auc: 0.83972 | train_accuracy: 0.75653 | valid_auc: 0.80927 | valid_accuracy: 0.75192 |  0:01:04s\n",
      "epoch 2  | loss: 0.48637 | train_auc: 0.84742 | train_accuracy: 0.76737 | valid_auc: 0.81726 | valid_accuracy: 0.70367 |  0:01:37s\n",
      "epoch 3  | loss: 0.4855  | train_auc: 0.84051 | train_accuracy: 0.75933 | valid_auc: 0.8055  | valid_accuracy: 0.74141 |  0:02:09s\n",
      "epoch 4  | loss: 0.4837  | train_auc: 0.85602 | train_accuracy: 0.77453 | valid_auc: 0.8137  | valid_accuracy: 0.7284  |  0:02:42s\n",
      "epoch 5  | loss: 0.4651  | train_auc: 0.86908 | train_accuracy: 0.78255 | valid_auc: 0.8164  | valid_accuracy: 0.73602 |  0:03:14s\n",
      "epoch 6  | loss: 0.44169 | train_auc: 0.87589 | train_accuracy: 0.78744 | valid_auc: 0.81707 | valid_accuracy: 0.76075 |  0:03:45s\n",
      "epoch 7  | loss: 0.43213 | train_auc: 0.87594 | train_accuracy: 0.78827 | valid_auc: 0.81859 | valid_accuracy: 0.75392 |  0:04:16s\n",
      "epoch 8  | loss: 0.43265 | train_auc: 0.6969  | train_accuracy: 0.64641 | valid_auc: 0.60493 | valid_accuracy: 0.46988 |  0:04:48s\n",
      "epoch 9  | loss: 0.42632 | train_auc: 0.8884  | train_accuracy: 0.7971  | valid_auc: 0.82081 | valid_accuracy: 0.78477 |  0:05:21s\n",
      "epoch 10 | loss: 0.41784 | train_auc: 0.89116 | train_accuracy: 0.80001 | valid_auc: 0.8202  | valid_accuracy: 0.77633 |  0:05:53s\n",
      "epoch 11 | loss: 0.41051 | train_auc: 0.8953  | train_accuracy: 0.80279 | valid_auc: 0.82002 | valid_accuracy: 0.76427 |  0:06:26s\n",
      "epoch 12 | loss: 0.40827 | train_auc: 0.78817 | train_accuracy: 0.68663 | valid_auc: 0.68089 | valid_accuracy: 0.5251  |  0:06:58s\n",
      "epoch 13 | loss: 0.40922 | train_auc: 0.89442 | train_accuracy: 0.80183 | valid_auc: 0.82079 | valid_accuracy: 0.76104 |  0:07:30s\n",
      "epoch 14 | loss: 0.40699 | train_auc: 0.88497 | train_accuracy: 0.76216 | valid_auc: 0.80555 | valid_accuracy: 0.6592  |  0:08:03s\n",
      "epoch 15 | loss: 0.40043 | train_auc: 0.88301 | train_accuracy: 0.78392 | valid_auc: 0.78729 | valid_accuracy: 0.71936 |  0:08:35s\n",
      "epoch 16 | loss: 0.39972 | train_auc: 0.89884 | train_accuracy: 0.80547 | valid_auc: 0.81607 | valid_accuracy: 0.76364 |  0:09:08s\n",
      "epoch 17 | loss: 0.39867 | train_auc: 0.89275 | train_accuracy: 0.79529 | valid_auc: 0.80488 | valid_accuracy: 0.73426 |  0:09:39s\n",
      "epoch 18 | loss: 0.39583 | train_auc: 0.90297 | train_accuracy: 0.81186 | valid_auc: 0.81448 | valid_accuracy: 0.78377 |  0:10:11s\n",
      "epoch 19 | loss: 0.3932  | train_auc: 0.90383 | train_accuracy: 0.81117 | valid_auc: 0.81232 | valid_accuracy: 0.78317 |  0:10:44s\n",
      "epoch 20 | loss: 0.38975 | train_auc: 0.82046 | train_accuracy: 0.70883 | valid_auc: 0.68619 | valid_accuracy: 0.55837 |  0:11:16s\n",
      "epoch 21 | loss: 0.38856 | train_auc: 0.90228 | train_accuracy: 0.80952 | valid_auc: 0.81461 | valid_accuracy: 0.77654 |  0:11:49s\n",
      "epoch 22 | loss: 0.38676 | train_auc: 0.8763  | train_accuracy: 0.7503  | valid_auc: 0.7967  | valid_accuracy: 0.62641 |  0:12:21s\n",
      "epoch 23 | loss: 0.38558 | train_auc: 0.83277 | train_accuracy: 0.74363 | valid_auc: 0.69786 | valid_accuracy: 0.6541  |  0:12:53s\n",
      "epoch 24 | loss: 0.38556 | train_auc: 0.63395 | train_accuracy: 0.58222 | valid_auc: 0.45973 | valid_accuracy: 0.31951 |  0:13:26s\n",
      "epoch 25 | loss: 0.38356 | train_auc: 0.88774 | train_accuracy: 0.74877 | valid_auc: 0.79552 | valid_accuracy: 0.6226  |  0:13:58s\n",
      "epoch 26 | loss: 0.38152 | train_auc: 0.89725 | train_accuracy: 0.80109 | valid_auc: 0.80429 | valid_accuracy: 0.75    |  0:14:30s\n",
      "epoch 27 | loss: 0.3824  | train_auc: 0.78836 | train_accuracy: 0.63546 | valid_auc: 0.63818 | valid_accuracy: 0.39165 |  0:15:02s\n",
      "epoch 28 | loss: 0.38043 | train_auc: 0.88111 | train_accuracy: 0.77163 | valid_auc: 0.75688 | valid_accuracy: 0.66477 |  0:15:33s\n",
      "epoch 29 | loss: 0.38015 | train_auc: 0.852   | train_accuracy: 0.69311 | valid_auc: 0.7486  | valid_accuracy: 0.52271 |  0:16:04s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 9 and best_valid_accuracy = 0.78477\n",
      "TabNet final trained in 16.42 mins\n",
      "\n",
      "--- TabNet Metrics ---\n",
      "Accuracy    : 0.8069\n",
      "Precision   : 0.3762\n",
      "Recall      : 0.5860\n",
      "F1-Score    : 0.4582\n",
      "ROC-AUC     : 0.8228\n",
      "Sensitivity : 0.5860\n",
      "Specificity : 0.8427\n",
      "PPV         : 0.3762\n",
      "NPV         : 0.9263\n"
     ]
    }
   ],
   "source": [
    "# === TRAIN FINAL TABNET MODEL AND EVALUATE ===\n",
    "\n",
    "best_tab = study_tab.best_trial.params\n",
    "clf_tabnet_final = TabNetClassifier(\n",
    "    n_d=best_tab.get('n_d', 8),\n",
    "    n_a=best_tab.get('n_a', 8),\n",
    "    n_steps=best_tab.get('n_steps', 3),\n",
    "    gamma=best_tab.get('gamma', 1.3),\n",
    "    lambda_sparse=best_tab.get('lambda_sparse', 0.001),\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=best_tab.get('lr', 0.02)),\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    scheduler_params={\"step_size\": 10, \"gamma\": 0.5},\n",
    "    mask_type='entmax',\n",
    "    device_name=DEVICE,\n",
    "    seed=SEED,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "clf_tabnet_final.fit(\n",
    "    X_train=X_train_bal, y_train=y_train_bal,\n",
    "    eval_set=[(X_train_bal, y_train_bal), (X_val_scaled, y_val)],\n",
    "    eval_name=['train', 'valid'],\n",
    "    eval_metric=['auc', 'accuracy'],\n",
    "    max_epochs=200,\n",
    "    patience=20,\n",
    "    batch_size=best_tab.get('batch_size', 256),\n",
    "    virtual_batch_size=min(128, best_tab.get('batch_size', 256)),\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"TabNet final trained in {(end-start)/60:.2f} mins\")\n",
    "\n",
    "# Evaluate TabNet\n",
    "tab_prob_val = clf_tabnet_final.predict_proba(X_val_scaled)[:, 1]\n",
    "tab_best_th = find_best_threshold(y_val, tab_prob_val)\n",
    "\n",
    "y_prob_tab = clf_tabnet_final.predict_proba(X_test_scaled)[:, 1]\n",
    "y_pred_tab = (y_prob_tab > tab_best_th).astype(int)\n",
    "\n",
    "tab_metrics = calculate_metrics(y_test, y_pred_tab, y_prob_tab)\n",
    "print('\\n--- TabNet Metrics ---')\n",
    "for k, v in tab_metrics.items():\n",
    "    print(f\"{k:<12}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45956d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Summary ---\n",
      "     Model  Accuracy  Precision    Recall  F1-Score   ROC-AUC  Sensitivity  \\\n",
      "0  XGBoost  0.817276   0.391052  0.558846  0.460129  0.826541     0.558846   \n",
      "1   TabNet  0.806896   0.376150  0.586005  0.458192  0.822811     0.586005   \n",
      "\n",
      "   Specificity       PPV       NPV  \n",
      "0     0.859115  0.391052  0.923249  \n",
      "1     0.842656  0.376150  0.926323  \n",
      "\n",
      "Saved: optuna_xgb_best.json, optuna_tabnet_best.json, tuning_metrics_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Summary DataFrame ---\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'TabNet'],\n",
    "    'Accuracy': [xgb_metrics['Accuracy'], tab_metrics['Accuracy']],\n",
    "    'Precision': [xgb_metrics['Precision'], tab_metrics['Precision']],\n",
    "    'Recall': [xgb_metrics['Recall'], tab_metrics['Recall']],\n",
    "    'F1-Score': [xgb_metrics['F1-Score'], tab_metrics['F1-Score']],\n",
    "    'ROC-AUC': [xgb_metrics['ROC-AUC'], tab_metrics['ROC-AUC']],\n",
    "    'Sensitivity': [xgb_metrics['Sensitivity'], tab_metrics['Sensitivity']],\n",
    "    'Specificity': [xgb_metrics['Specificity'], tab_metrics['Specificity']],\n",
    "    'PPV': [xgb_metrics['PPV'], tab_metrics['PPV']],\n",
    "    'NPV': [xgb_metrics['NPV'], tab_metrics['NPV']]\n",
    "})\n",
    "\n",
    "print('\\n--- Summary ---')\n",
    "print(metrics_df)\n",
    "\n",
    "# Save metrics\n",
    "metrics_df.to_csv('tuning_metrics_summary.csv', index=False)\n",
    "\n",
    "print('\\nSaved: optuna_xgb_best.json, optuna_tabnet_best.json, tuning_metrics_summary.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venve (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
